{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa7ffdf-967e-4781-ac3d-f37ec16b62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88df70-4fcf-48b8-8189-8513b67f04a2",
   "metadata": {},
   "source": [
    "# Déf de l'env 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0dfbf4c-ec7d-4c4c-bf5d-0a9584176a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class KubernetesSchedulerEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environnement simulé pour l'ordonnancement de pods Kubernetes\n",
    "    Compatible Gymnasium - Version simplifiée pour PPO\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, num_nodes=10, num_pods=20, pod_list=None,\n",
    "                 cpu_req_range=(0.5, 4.0), mem_req_range=(1.0, 16.0), seed=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_pods = num_pods\n",
    "        self.max_cpu_per_node = 8.0\n",
    "        self.max_memory_per_node = 32.0\n",
    "\n",
    "        self.pod_list = pod_list\n",
    "        self.cpu_req_range = cpu_req_range\n",
    "        self.mem_req_range = mem_req_range\n",
    "        self.seed_value = seed\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        obs_size = num_nodes * 4 + 2 + 2  # nodes + pod courant + stats\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(obs_size,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(num_nodes + 1)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            self.seed_value = seed\n",
    "\n",
    "        # Nodes\n",
    "        self.nodes = np.zeros((self.num_nodes, 4), dtype=np.float32)\n",
    "        for i in range(self.num_nodes):\n",
    "            self.nodes[i] = [0, 0, self.max_cpu_per_node, self.max_memory_per_node]\n",
    "\n",
    "        # Pods\n",
    "        self.pending_pods = []\n",
    "        if self.pod_list is not None:\n",
    "            self.pending_pods = [list(pod) for pod in self.pod_list]\n",
    "        else:\n",
    "            for _ in range(self.num_pods):\n",
    "                cpu_req = np.random.uniform(*self.cpu_req_range)\n",
    "                mem_req = np.random.uniform(*self.mem_req_range)\n",
    "                self.pending_pods.append([cpu_req, mem_req])\n",
    "\n",
    "        self.current_pod_idx = 0\n",
    "        self.total_reward = 0\n",
    "        self.successful_placements = 0\n",
    "        self.steps_taken = 0\n",
    "\n",
    "        return self._get_observation().astype(np.float32), {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        nodes_state = []\n",
    "        for n in self.nodes:\n",
    "            cpu_used, mem_used, cpu_total, mem_total = n\n",
    "            cpu_available = cpu_total - cpu_used\n",
    "            mem_available = mem_total - mem_used\n",
    "            nodes_state.extend([\n",
    "                cpu_used / cpu_total,\n",
    "                mem_used / mem_total,\n",
    "                cpu_available / self.max_cpu_per_node,\n",
    "                mem_available / self.max_memory_per_node\n",
    "            ])\n",
    "\n",
    "        pod_state = [0, 0]\n",
    "        if self.current_pod_idx < len(self.pending_pods):\n",
    "            cpu_req, mem_req = self.pending_pods[self.current_pod_idx]\n",
    "            pod_state = [cpu_req / self.max_cpu_per_node, mem_req / self.max_memory_per_node]\n",
    "\n",
    "        global_stats = [\n",
    "            self.current_pod_idx / max(len(self.pending_pods), 1),\n",
    "            self.successful_placements / max(len(self.pending_pods), 1)\n",
    "        ]\n",
    "\n",
    "        return np.array(nodes_state + pod_state + global_stats, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "        info = {\"placement\": \"none\"}\n",
    "\n",
    "        if self.current_pod_idx >= len(self.pending_pods):\n",
    "            terminated = True\n",
    "            return self._get_observation().astype(np.float32), reward, terminated, truncated, info\n",
    "\n",
    "        cpu_req, mem_req = self.pending_pods[self.current_pod_idx]\n",
    "\n",
    "        if action < self.num_nodes:\n",
    "            node = self.nodes[action]\n",
    "            cpu_used, mem_used, cpu_total, mem_total = node\n",
    "            if cpu_used + cpu_req <= cpu_total and mem_used + mem_req <= mem_total:\n",
    "                self.nodes[action][0] += cpu_req\n",
    "                self.nodes[action][1] += mem_req\n",
    "                self.successful_placements += 1\n",
    "                cpu_util = (cpu_used + cpu_req) / cpu_total\n",
    "                mem_util = (mem_used + mem_req) / mem_total\n",
    "                balance = 1.0 - abs(cpu_util - mem_util)\n",
    "                avg_util = (cpu_util + mem_util) / 2\n",
    "                reward = 1.0 + 0.3 * balance + 0.2 * avg_util\n",
    "                info[\"placement\"] = \"success\"\n",
    "            else:\n",
    "                reward = -0.3\n",
    "                info[\"placement\"] = \"failed\"\n",
    "        else:\n",
    "            reward = -0.2\n",
    "            info[\"placement\"] = \"skipped\"\n",
    "\n",
    "        self.current_pod_idx += 1\n",
    "        self.steps_taken += 1\n",
    "        self.total_reward += reward\n",
    "\n",
    "        if self.current_pod_idx >= len(self.pending_pods):\n",
    "            terminated = True\n",
    "            success_rate = self.successful_placements / len(self.pending_pods)\n",
    "            reward += 2.0 * success_rate\n",
    "            info[\"final_success_rate\"] = success_rate\n",
    "            info[\"total_reward\"] = self.total_reward\n",
    "            info[\"placement_rate\"] = success_rate\n",
    "\n",
    "        return self._get_observation().astype(np.float32), reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        print(f\"\\n=== Épisode ===\")\n",
    "        print(f\"Pods traités: {self.current_pod_idx}/{len(self.pending_pods)}\")\n",
    "        print(f\"Pods placés avec succès: {self.successful_placements}/{len(self.pending_pods)}\")\n",
    "        print(f\"Récompense totale: {self.total_reward:.2f}\")\n",
    "        if self.current_pod_idx >= len(self.pending_pods):\n",
    "            success_rate = self.successful_placements / len(self.pending_pods)\n",
    "            print(f\"Taux de placement final: {success_rate:.1%}\")\n",
    "        for i, node in enumerate(self.nodes):\n",
    "            cpu_used, mem_used, cpu_total, mem_total = node\n",
    "            print(f\"Node {i}: CPU {cpu_used:.1f}/{cpu_total:.1f}, MEM {mem_used:.1f}/{mem_total:.1f}\")\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {\n",
    "            \"total_reward\": self.total_reward,\n",
    "            \"successful_placements\": self.successful_placements,\n",
    "            \"total_pods\": len(self.pending_pods),\n",
    "            \"placement_rate\": self.successful_placements / len(self.pending_pods) if len(self.pending_pods) > 0 else 0,\n",
    "            \"steps_taken\": self.steps_taken\n",
    "        }\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d62b62-f797-43c4-8232-03b2dba41dc4",
   "metadata": {},
   "source": [
    "# Agent PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97da044-346b-44cf-9d9b-3d36e91d95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "class PPOSchedulerAgent:\n",
    "    def __init__(self, env, model_name=\"ppo_k8s_scheduler\", save_path=\"results/models/\"):\n",
    "        self.env = DummyVecEnv([lambda: Monitor(env)])\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.best_model_path = None\n",
    "        self.save_path = save_path\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        check_env(env, warn=True)\n",
    "\n",
    "    def train(self, total_timesteps=50000):\n",
    "        self.model = PPO(\n",
    "            \"MlpPolicy\",\n",
    "            self.env,\n",
    "            verbose=1,\n",
    "            learning_rate=2.5e-4,\n",
    "            n_steps=2048,\n",
    "            batch_size=64,\n",
    "            n_epochs=10,\n",
    "            gamma=0.99,\n",
    "            gae_lambda=0.95,\n",
    "            clip_range=0.2,\n",
    "            ent_coef=0.01,\n",
    "            vf_coef=0.5,\n",
    "            max_grad_norm=0.5,\n",
    "            tensorboard_log=\"./ppo_k8s_tensorboard/\"\n",
    "        )\n",
    "        self.model.learn(total_timesteps=total_timesteps)\n",
    "        final_model_path = os.path.join(self.save_path, f\"{self.model_name}_final.zip\")\n",
    "        self.model.save(final_model_path)\n",
    "        self.best_model_path = final_model_path\n",
    "        print(f\"Modèle sauvegardé : {final_model_path}\")\n",
    "\n",
    "    def load(self, model_path=None, best_model=False):\n",
    "        if best_model:\n",
    "            model_path = self.best_model_path\n",
    "        if model_path is None:\n",
    "            raise ValueError(\"Aucun modèle spécifié pour le chargement.\")\n",
    "        self.model = PPO.load(model_path, env=self.env)\n",
    "        print(f\"Modèle chargé : {model_path}\")\n",
    "\n",
    "    def predict(self, observation, deterministic=True):\n",
    "        if observation.ndim == 1:\n",
    "            observation = observation[None, :]\n",
    "        action, _ = self.model.predict(observation, deterministic=deterministic)\n",
    "        return action[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114523c0-5c7f-4500-b75c-43928478874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ppo_agent(env, agent, nb_episodes=10, render_last=False):\n",
    "    rewards, placements = [], []\n",
    "\n",
    "    for ep in range(nb_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        successful = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if info.get(\"placement\") == \"success\":\n",
    "                successful += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "        placement_rate = successful / env.num_pods\n",
    "        rewards.append(total_reward)\n",
    "        placements.append(placement_rate)\n",
    "\n",
    "        if render_last and ep == nb_episodes - 1:\n",
    "            env.render()\n",
    "\n",
    "    mean_reward = float(np.mean(rewards))\n",
    "    std_reward = float(np.std(rewards))\n",
    "    mean_placement = float(np.mean(placements))\n",
    "    std_placement = float(np.std(placements))\n",
    "\n",
    "    print(f\"\\nReward moyen : {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "    print(f\"Placement moyen : {mean_placement:.1%} ± {std_placement:.1%}\")\n",
    "\n",
    "    return mean_reward, mean_placement, std_reward, std_placement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5058fc5-d551-4b4b-a5f0-7c82cdf95d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_k8s_tensorboard/PPO_4\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 284      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20          |\n",
      "|    ep_rew_mean          | 22.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 218         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011316694 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | -0.00601    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.42        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20          |\n",
      "|    ep_rew_mean          | 23.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013011262 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.38       |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 6.02        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20          |\n",
      "|    ep_rew_mean          | 24.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010432663 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.35       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 3.14        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 20        |\n",
      "|    ep_rew_mean          | 24.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 186       |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 54        |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0106414 |\n",
      "|    clip_fraction        | 0.107     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.33     |\n",
      "|    explained_variance   | 0.937     |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 0.539     |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.0191   |\n",
      "|    value_loss           | 1.71      |\n",
      "---------------------------------------\n",
      "Modèle sauvegardé : best_model/ppo_k8s_scheduler_final.zip\n",
      "Modèle chargé : best_model/ppo_k8s_scheduler_final.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# --- Paramètres ---\n",
    "num_episodes = 500   # Nombre d'épisodes pour le calcul des timesteps\n",
    "num_pods = 20\n",
    "save_path = \"best_model/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# --- Création de l'environnement ---\n",
    "env_train = KubernetesSchedulerEnv(num_nodes=10, num_pods=num_pods, seed=42)\n",
    "\n",
    "# --- Vérification de l'environnement ---\n",
    "check_env(env_train, warn=True)\n",
    "\n",
    "# --- Création et entraînement de l'agent ---\n",
    "ppo_agent = PPOSchedulerAgent(env_train, save_path=save_path)\n",
    "\n",
    "# Calcul automatique du nombre total de timesteps\n",
    "total_timesteps = num_episodes * num_pods  # nb épisodes x nb pods\n",
    "\n",
    "# Entraînement\n",
    "ppo_agent.train(total_timesteps=total_timesteps)\n",
    "\n",
    "# Chargement du meilleur modèle après entraînement\n",
    "ppo_agent.load(best_model=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d60d5dc1-1cc9-4a92-b631-63ffb16194dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Épisode ===\n",
      "Pods traités: 20/20\n",
      "Pods placés avec succès: 18/20\n",
      "Récompense totale: 23.27\n",
      "Taux de placement final: 90.0%\n",
      "Node 0: CPU 5.2/8.0, MEM 5.5/32.0\n",
      "Node 1: CPU 3.5/8.0, MEM 11.4/32.0\n",
      "Node 2: CPU 2.6/8.0, MEM 7.5/32.0\n",
      "Node 3: CPU 4.7/8.0, MEM 7.4/32.0\n",
      "Node 4: CPU 4.6/8.0, MEM 11.4/32.0\n",
      "Node 5: CPU 6.7/8.0, MEM 28.7/32.0\n",
      "Node 6: CPU 5.7/8.0, MEM 17.8/32.0\n",
      "Node 7: CPU 2.9/8.0, MEM 18.3/32.0\n",
      "Node 8: CPU 3.1/8.0, MEM 11.2/32.0\n",
      "Node 9: CPU 5.9/8.0, MEM 5.3/32.0\n",
      "\n",
      "Reward moyen : 24.60 ± 2.20\n",
      "Placement moyen : 88.3% ± 6.2%\n"
     ]
    }
   ],
   "source": [
    "# Évaluation\n",
    "mean_reward, mean_placement, std_reward, std_placement = evaluate_ppo_agent(\n",
    "    env_train, ppo_agent, nb_episodes=3, render_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "845879f3-4990-4075-9375-5133575635ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_scheduler_policy(env, max_pods_per_node=3, node_failure_prob=0.1):\n",
    "    \"\"\"Place les pods sur le premier nœud disponible avec contraintes réalistes.\"\"\"\n",
    "    node_pod_count = np.zeros(env.num_nodes, dtype=int)\n",
    "    \n",
    "    while env.current_pod_idx < env.num_pods:\n",
    "        cpu_req, mem_req = env.pending_pods[env.current_pod_idx]\n",
    "        action = env.num_nodes  # skip par défaut\n",
    "        for n in range(env.num_nodes):\n",
    "            cpu_used, mem_used, cpu_total, mem_total = env.nodes[n]\n",
    "            \n",
    "            if (cpu_used + cpu_req <= cpu_total and\n",
    "                mem_used + mem_req <= mem_total and\n",
    "                node_pod_count[n] < max_pods_per_node and\n",
    "                np.random.rand() > node_failure_prob):\n",
    "                action = n\n",
    "                node_pod_count[n] += 1\n",
    "                break\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "378a2508-b9f3-4563-9f72-2d0752cb613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_scheduler_policy(env):\n",
    "    for pod_idx in range(env.num_pods):\n",
    "        action = np.random.randint(0, env.num_nodes + 1)\n",
    "        env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f781f24-afa9-4958-9ad5-cd906040ca12",
   "metadata": {},
   "source": [
    "# Fonction de comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7157df78-cc91-4d77-8daa-aab21bc83b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Fonction de comparaison\n",
    "# --------------------------\n",
    "def compare_schedulers(env, ppo_agent, nb_episodes=10):\n",
    "    results = {\n",
    "        \"PPO\": {\"reward\": [], \"placement\": []},\n",
    "        \"Random\": {\"reward\": [], \"placement\": []},\n",
    "        \"Default\": {\"reward\": [], \"placement\": []}\n",
    "    }\n",
    "\n",
    "    for ep in range(nb_episodes):\n",
    "        # --- PPO ---\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        successful = 0\n",
    "        while not done:\n",
    "            action = ppo_agent.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if info.get(\"placement\") == \"success\":\n",
    "                successful += 1\n",
    "            done = terminated or truncated\n",
    "        results[\"PPO\"][\"reward\"].append(total_reward)\n",
    "        results[\"PPO\"][\"placement\"].append(successful / env.num_pods)\n",
    "\n",
    "        # --- Random ---\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        successful = 0\n",
    "        while not done:\n",
    "            action = np.random.randint(0, env.num_nodes + 1)  # choisir aléatoirement node ou skip\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if info.get(\"placement\") == \"success\":\n",
    "                successful += 1\n",
    "            done = terminated or truncated\n",
    "        results[\"Random\"][\"reward\"].append(total_reward)\n",
    "        results[\"Random\"][\"placement\"].append(successful / env.num_pods)\n",
    "\n",
    "        # --- Default scheduler (ex: placement premier node possible) ---\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        successful = 0\n",
    "        while not done:\n",
    "            cpu_req, mem_req = env.pending_pods[env.current_pod_idx]\n",
    "            # trouver le premier node capable de placer le pod\n",
    "            action = env.num_nodes  # skip par défaut\n",
    "            for n in range(env.num_nodes):\n",
    "                cpu_used, mem_used, cpu_total, mem_total = env.nodes[n]\n",
    "                if cpu_used + cpu_req <= cpu_total and mem_used + mem_req <= mem_total:\n",
    "                    action = n\n",
    "                    break\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if info.get(\"placement\") == \"success\":\n",
    "                successful += 1\n",
    "            done = terminated or truncated\n",
    "        results[\"Default\"][\"reward\"].append(total_reward)\n",
    "        results[\"Default\"][\"placement\"].append(successful / env.num_pods)\n",
    "\n",
    "    # Affichage résumé\n",
    "    print(\"\\n=== COMPARAISON DES SCHEDULERS ===\")\n",
    "    for sched in results:\n",
    "        r_mean = np.mean(results[sched][\"reward\"])\n",
    "        r_std = np.std(results[sched][\"reward\"])\n",
    "        p_mean = np.mean(results[sched][\"placement\"])\n",
    "        p_std = np.std(results[sched][\"placement\"])\n",
    "        print(f\"{sched}: Reward moyen={r_mean:.2f}±{r_std:.2f}, \"\n",
    "              f\"Placement moyen={p_mean:.2%}±{p_std:.2%}\")\n",
    "\n",
    "    return results\n",
   
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a871835f-edfd-413c-a8cb-508792b18b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé : best_model/ppo_k8s_scheduler_final.zip\n",
      "\n",
      "=== COMPARAISON DES SCHEDULERS ===\n",
      "PPO: Reward moyen=26.14±1.62, Placement moyen=93.00%±4.00%\n",
      "Random: Reward moyen=20.92±1.35, Placement moyen=77.00%±4.00%\n",
      "Default: Reward moyen=29.26±0.26, Placement moyen=100.00%±0.00%\n"
     ]
    }
   ],
   "source": [
    "ppo_agent.load(model_path=\"best_model/ppo_k8s_scheduler_final.zip\")\n",
    "compare_results = compare_schedulers(env_train, ppo_agent, nb_episodes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be435bb2-21b0-41fa-a62c-31bedd1ba2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_model.zip',\n",
       " 'best_model1.zip',\n",
       " 'logs',\n",
       " 'ppo_k8s_scheduler_final',\n",
       " 'ppo_k8s_scheduler_final.zip']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"best_model/\")  # Vérifie qu'il y a bien \"best_model.zip\" ou un autre fichier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225665fb-6248-4a10-bcec-fe68d74e893a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
